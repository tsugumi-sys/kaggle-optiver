{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "noble-provision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['log_return1_realized_volatility', 'log_return1_realized_volatility_150', 'stock_id', 'log_return2_realized_volatility', 'log_return1_realized_volatility_300', 'log_return2_realized_volatility_150', 'log_return2_realized_volatility_450', 'log_return2_realized_volatility_300', 'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450_mean_time', 'log_return2_realized_volatility_300_min_time', 'log_return2_realized_volatility_450_min_time', 'price_spread_mean_150', 'log_return2_realized_volatility_min_time', 'trade_log_return_realized_volatility_450_min_time', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_std_time', 'log_return2_realized_volatility_150_min_time', 'log_return1_realized_volatility_min_time', 'log_return1_realized_volatility_450_mean_stock', 'volume_imbalance_mean', 'trade_log_return_realized_volatility_300_min_time', 'price_spread_sum_300', 'price_spread_mean', 'log_return1_realized_volatility_450_mean_time', 'log_return2_realized_volatility_std_time', 'price_spread_mean_300', 'log_return2_realized_volatility_450_mean_time', 'log_return1_realized_volatility_150_min_time', 'log_return1_std_450', 'ask_spread_mean_450', 'log_return1_realized_volatility_mean_time', 'trade_log_return_realized_volatility_min_time', 'log_return1_realized_volatility_std_time', 'log_return1_realized_volatility_450', 'log_return1_std_150', 'log_return1_realized_volatility_mean_stock', 'trade_log_return_realized_volatility_mean_time', 'log_return1_realized_volatility_450_min_time', 'price_spread_sum', 'trade_log_return_realized_volatility_300_max_time', 'log_return1_realized_volatility_300_max_time', 'trade_log_return_realized_volatility_150_std_time', 'log_return1_realized_volatility_max_time', 'log_return1_std_300', 'log_return1_realized_volatility_450_max_time', 'log_return2_realized_volatility_300_max_time', 'log_return1_realized_volatility_300_mean_time', 'log_return1_realized_volatility_300_std_time', 'log_return1_realized_volatility_300_min_time', 'trade_log_return_realized_volatility_450_max_time', 'trade_log_return_realized_volatility_max_time', 'log_return1_realized_volatility_450_std_time', 'trade_log_return_realized_volatility_450_std_time', 'trade_log_return_realized_volatility_300_mean_time', 'log_return2_realized_volatility_max_time', 'trade_log_return_realized_volatility_150_min_time', 'log_return2_realized_volatility_150_max_time', 'bid_spread_mean_450', 'trade_log_return_realized_volatility_300_std_time', 'log_return2_realized_volatility_450_max_time', 'log_return2_realized_volatility_450_std_time', 'log_return2_realized_volatility_300_std_time', 'log_return1_realized_volatility_150_max_time', 'log_return2_realized_volatility_300_mean_time', 'trade_log_return_realized_volatility_150', 'log_return2_realized_volatility_150_std_time', 'wap_balance_mean_300', 'log_return1_realized_volatility_150_std_time', 'log_return2_realized_volatility_mean_time', 'price_spread_sum_450', 'wap_balance_sum_150', 'log_return1_realized_volatility_150_mean_time', 'log_return1_realized_volatility_min_stock', 'log_return2_std_450', 'log_return2_std_150', 'trade_log_return_realized_volatility_150_mean_time', 'trade_log_return_realized_volatility_150_max_time', 'total_volume_std', 'log_return2_realized_volatility_150_mean_time', 'price_spread_mean_450', 'log_return1_std', 'total_volume_sum', 'trade_seconds_in_bucket_count_unique_450', 'trade_log_return_realized_volatility_300', 'wap_balance_mean_150', 'wap_balance_sum', 'volume_imbalance_mean_150', 'log_return1_realized_volatility_300_mean_stock', 'price_spread_std', 'total_volume_mean', 'volume_imbalance_std_150', 'wap2_std_450', 'trade_seconds_in_bucket_count_unique', 'price_spread_sum_150', 'trade_seconds_in_bucket_count_unique_150', 'log_return1_realized_volatility_150_min_stock', 'trade_order_count_mean_450', 'wap1_std_450', 'total_volume_std_150', 'log_return2_realized_volatility_mean_stock', 'wap1_std_300', 'trade_log_return_realized_volatility_450_mean_stock', 'trade_order_count_mean', 'trade_size_sum_450', 'wap1_sum', 'wap_balance_sum_300', 'log_return1_sum', 'wap2_std_300', 'trade_order_count_mean_300', 'trade_seconds_in_bucket_count_unique_300', 'log_return2_sum', 'trade_order_count_mean_150', 'bid_spread_std', 'wap_balance_sum_450', 'wap1_std_150', 'wap_balance_mean_450', 'wap2_mean_450', 'trade_size_sum_150', 'wap2_sum', 'price_spread_std_300', 'wap_balance_std', 'total_volume_std_300', 'trade_size_sum_300']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', 300)\n",
    "feature_name_file = 'features_gain_importance_50percent.csv'\n",
    "selected_feature_names = pd.read_csv(feature_name_file)['feature_name'].values.tolist()\n",
    "print(selected_feature_names)\n",
    "\n",
    "data_dir = '../../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "younger-titanium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   56.2s\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:  3.4min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428932, 124) (3, 124)\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000474574\ttraining's RMSPE: 0.219802\tvalid_1's rmse: 0.000488157\tvalid_1's RMSPE: 0.225252\n",
      "[100]\ttraining's rmse: 0.000454296\ttraining's RMSPE: 0.21041\tvalid_1's rmse: 0.000470646\tvalid_1's RMSPE: 0.217172\n",
      "[150]\ttraining's rmse: 0.00044108\ttraining's RMSPE: 0.204289\tvalid_1's rmse: 0.000459638\tvalid_1's RMSPE: 0.212093\n",
      "[200]\ttraining's rmse: 0.000431646\ttraining's RMSPE: 0.19992\tvalid_1's rmse: 0.000453911\tvalid_1's RMSPE: 0.20945\n",
      "[250]\ttraining's rmse: 0.000424487\ttraining's RMSPE: 0.196604\tvalid_1's rmse: 0.000449256\tvalid_1's RMSPE: 0.207302\n",
      "[300]\ttraining's rmse: 0.000417309\ttraining's RMSPE: 0.193279\tvalid_1's rmse: 0.000444144\tvalid_1's RMSPE: 0.204943\n",
      "[350]\ttraining's rmse: 0.000411783\ttraining's RMSPE: 0.19072\tvalid_1's rmse: 0.000441655\tvalid_1's RMSPE: 0.203795\n",
      "[400]\ttraining's rmse: 0.000406989\ttraining's RMSPE: 0.1885\tvalid_1's rmse: 0.000439174\tvalid_1's RMSPE: 0.20265\n",
      "[450]\ttraining's rmse: 0.000402729\ttraining's RMSPE: 0.186526\tvalid_1's rmse: 0.000437003\tvalid_1's RMSPE: 0.201648\n",
      "[500]\ttraining's rmse: 0.000398166\ttraining's RMSPE: 0.184413\tvalid_1's rmse: 0.000434845\tvalid_1's RMSPE: 0.200652\n",
      "[550]\ttraining's rmse: 0.000394476\ttraining's RMSPE: 0.182704\tvalid_1's rmse: 0.000433063\tvalid_1's RMSPE: 0.19983\n",
      "[600]\ttraining's rmse: 0.000390771\ttraining's RMSPE: 0.180988\tvalid_1's rmse: 0.00043191\tvalid_1's RMSPE: 0.199298\n",
      "[650]\ttraining's rmse: 0.000387084\ttraining's RMSPE: 0.179281\tvalid_1's rmse: 0.000430504\tvalid_1's RMSPE: 0.198649\n",
      "[700]\ttraining's rmse: 0.000383915\ttraining's RMSPE: 0.177813\tvalid_1's rmse: 0.00042975\tvalid_1's RMSPE: 0.198301\n",
      "[750]\ttraining's rmse: 0.000381007\ttraining's RMSPE: 0.176466\tvalid_1's rmse: 0.000429191\tvalid_1's RMSPE: 0.198043\n",
      "[800]\ttraining's rmse: 0.000378478\ttraining's RMSPE: 0.175295\tvalid_1's rmse: 0.00042897\tvalid_1's RMSPE: 0.197941\n",
      "[850]\ttraining's rmse: 0.000375961\ttraining's RMSPE: 0.174129\tvalid_1's rmse: 0.000428589\tvalid_1's RMSPE: 0.197766\n",
      "[900]\ttraining's rmse: 0.000373622\ttraining's RMSPE: 0.173046\tvalid_1's rmse: 0.000427936\tvalid_1's RMSPE: 0.197464\n",
      "[950]\ttraining's rmse: 0.000371247\ttraining's RMSPE: 0.171945\tvalid_1's rmse: 0.000427304\tvalid_1's RMSPE: 0.197173\n",
      "[1000]\ttraining's rmse: 0.000369115\ttraining's RMSPE: 0.170958\tvalid_1's rmse: 0.000426721\tvalid_1's RMSPE: 0.196904\n",
      "[1050]\ttraining's rmse: 0.000366884\ttraining's RMSPE: 0.169925\tvalid_1's rmse: 0.000425966\tvalid_1's RMSPE: 0.196556\n",
      "[1100]\ttraining's rmse: 0.000364909\ttraining's RMSPE: 0.16901\tvalid_1's rmse: 0.000425502\tvalid_1's RMSPE: 0.196341\n",
      "[1150]\ttraining's rmse: 0.000362883\ttraining's RMSPE: 0.168072\tvalid_1's rmse: 0.000425857\tvalid_1's RMSPE: 0.196505\n",
      "Early stopping, best iteration is:\n",
      "[1116]\ttraining's rmse: 0.000364271\ttraining's RMSPE: 0.168715\tvalid_1's rmse: 0.00042541\tvalid_1's RMSPE: 0.196299\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000475859\ttraining's RMSPE: 0.220117\tvalid_1's rmse: 0.000481284\tvalid_1's RMSPE: 0.223216\n",
      "[100]\ttraining's rmse: 0.000455927\ttraining's RMSPE: 0.210897\tvalid_1's rmse: 0.000464824\tvalid_1's RMSPE: 0.215582\n",
      "[150]\ttraining's rmse: 0.000442568\ttraining's RMSPE: 0.204717\tvalid_1's rmse: 0.000454086\tvalid_1's RMSPE: 0.210602\n",
      "[200]\ttraining's rmse: 0.000432949\ttraining's RMSPE: 0.200268\tvalid_1's rmse: 0.000446912\tvalid_1's RMSPE: 0.207274\n",
      "[250]\ttraining's rmse: 0.000424442\ttraining's RMSPE: 0.196333\tvalid_1's rmse: 0.000441738\tvalid_1's RMSPE: 0.204875\n",
      "[300]\ttraining's rmse: 0.00041779\ttraining's RMSPE: 0.193256\tvalid_1's rmse: 0.000438038\tvalid_1's RMSPE: 0.203159\n",
      "[350]\ttraining's rmse: 0.000411952\ttraining's RMSPE: 0.190555\tvalid_1's rmse: 0.000435004\tvalid_1's RMSPE: 0.201752\n",
      "[400]\ttraining's rmse: 0.000406966\ttraining's RMSPE: 0.188249\tvalid_1's rmse: 0.000432565\tvalid_1's RMSPE: 0.200621\n",
      "[450]\ttraining's rmse: 0.000402516\ttraining's RMSPE: 0.186191\tvalid_1's rmse: 0.00043069\tvalid_1's RMSPE: 0.199751\n",
      "[500]\ttraining's rmse: 0.000398519\ttraining's RMSPE: 0.184342\tvalid_1's rmse: 0.000428854\tvalid_1's RMSPE: 0.1989\n",
      "[550]\ttraining's rmse: 0.000394311\ttraining's RMSPE: 0.182396\tvalid_1's rmse: 0.000427176\tvalid_1's RMSPE: 0.198121\n",
      "[600]\ttraining's rmse: 0.000390855\ttraining's RMSPE: 0.180797\tvalid_1's rmse: 0.00042609\tvalid_1's RMSPE: 0.197617\n",
      "[650]\ttraining's rmse: 0.000387401\ttraining's RMSPE: 0.179199\tvalid_1's rmse: 0.000424292\tvalid_1's RMSPE: 0.196784\n",
      "[700]\ttraining's rmse: 0.00038465\ttraining's RMSPE: 0.177927\tvalid_1's rmse: 0.000423708\tvalid_1's RMSPE: 0.196513\n",
      "[750]\ttraining's rmse: 0.000381804\ttraining's RMSPE: 0.17661\tvalid_1's rmse: 0.000423506\tvalid_1's RMSPE: 0.196419\n",
      "[800]\ttraining's rmse: 0.000379157\ttraining's RMSPE: 0.175386\tvalid_1's rmse: 0.000422875\tvalid_1's RMSPE: 0.196127\n",
      "[850]\ttraining's rmse: 0.00037653\ttraining's RMSPE: 0.174171\tvalid_1's rmse: 0.000421981\tvalid_1's RMSPE: 0.195712\n",
      "[900]\ttraining's rmse: 0.000373747\ttraining's RMSPE: 0.172883\tvalid_1's rmse: 0.000421032\tvalid_1's RMSPE: 0.195272\n",
      "[950]\ttraining's rmse: 0.000371346\ttraining's RMSPE: 0.171772\tvalid_1's rmse: 0.00042018\tvalid_1's RMSPE: 0.194876\n",
      "[1000]\ttraining's rmse: 0.000368952\ttraining's RMSPE: 0.170665\tvalid_1's rmse: 0.000419848\tvalid_1's RMSPE: 0.194722\n",
      "[1050]\ttraining's rmse: 0.000366896\ttraining's RMSPE: 0.169714\tvalid_1's rmse: 0.000419764\tvalid_1's RMSPE: 0.194684\n",
      "Early stopping, best iteration is:\n",
      "[1044]\ttraining's rmse: 0.00036716\ttraining's RMSPE: 0.169836\tvalid_1's rmse: 0.000419451\tvalid_1's RMSPE: 0.194538\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000475763\ttraining's RMSPE: 0.220207\tvalid_1's rmse: 0.000481979\tvalid_1's RMSPE: 0.222994\n",
      "[100]\ttraining's rmse: 0.000454253\ttraining's RMSPE: 0.210251\tvalid_1's rmse: 0.000464167\tvalid_1's RMSPE: 0.214753\n",
      "[150]\ttraining's rmse: 0.0004416\ttraining's RMSPE: 0.204395\tvalid_1's rmse: 0.000454615\tvalid_1's RMSPE: 0.210334\n",
      "[200]\ttraining's rmse: 0.000431947\ttraining's RMSPE: 0.199927\tvalid_1's rmse: 0.000447944\tvalid_1's RMSPE: 0.207248\n",
      "[250]\ttraining's rmse: 0.000423989\ttraining's RMSPE: 0.196243\tvalid_1's rmse: 0.000443364\tvalid_1's RMSPE: 0.205129\n",
      "[300]\ttraining's rmse: 0.000417397\ttraining's RMSPE: 0.193192\tvalid_1's rmse: 0.00043918\tvalid_1's RMSPE: 0.203193\n",
      "[350]\ttraining's rmse: 0.000411623\ttraining's RMSPE: 0.19052\tvalid_1's rmse: 0.000435922\tvalid_1's RMSPE: 0.201685\n",
      "[400]\ttraining's rmse: 0.000406686\ttraining's RMSPE: 0.188234\tvalid_1's rmse: 0.000434174\tvalid_1's RMSPE: 0.200877\n",
      "[450]\ttraining's rmse: 0.000401924\ttraining's RMSPE: 0.18603\tvalid_1's rmse: 0.000432094\tvalid_1's RMSPE: 0.199914\n",
      "[500]\ttraining's rmse: 0.000397879\ttraining's RMSPE: 0.184158\tvalid_1's rmse: 0.000430734\tvalid_1's RMSPE: 0.199285\n",
      "[550]\ttraining's rmse: 0.000393969\ttraining's RMSPE: 0.182349\tvalid_1's rmse: 0.000429382\tvalid_1's RMSPE: 0.19866\n",
      "[600]\ttraining's rmse: 0.000389911\ttraining's RMSPE: 0.18047\tvalid_1's rmse: 0.000427568\tvalid_1's RMSPE: 0.19782\n",
      "[650]\ttraining's rmse: 0.000386956\ttraining's RMSPE: 0.179103\tvalid_1's rmse: 0.000426686\tvalid_1's RMSPE: 0.197412\n",
      "[700]\ttraining's rmse: 0.000384001\ttraining's RMSPE: 0.177735\tvalid_1's rmse: 0.000426166\tvalid_1's RMSPE: 0.197171\n",
      "[750]\ttraining's rmse: 0.000381212\ttraining's RMSPE: 0.176444\tvalid_1's rmse: 0.000425637\tvalid_1's RMSPE: 0.196927\n",
      "[800]\ttraining's rmse: 0.000378291\ttraining's RMSPE: 0.175092\tvalid_1's rmse: 0.000425108\tvalid_1's RMSPE: 0.196682\n",
      "[850]\ttraining's rmse: 0.000375869\ttraining's RMSPE: 0.173971\tvalid_1's rmse: 0.000424669\tvalid_1's RMSPE: 0.196479\n",
      "[900]\ttraining's rmse: 0.000373243\ttraining's RMSPE: 0.172756\tvalid_1's rmse: 0.000424255\tvalid_1's RMSPE: 0.196287\n",
      "[950]\ttraining's rmse: 0.000371056\ttraining's RMSPE: 0.171743\tvalid_1's rmse: 0.00042392\tvalid_1's RMSPE: 0.196132\n",
      "[1000]\ttraining's rmse: 0.000368865\ttraining's RMSPE: 0.170729\tvalid_1's rmse: 0.000423802\tvalid_1's RMSPE: 0.196078\n",
      "[1050]\ttraining's rmse: 0.000366687\ttraining's RMSPE: 0.169721\tvalid_1's rmse: 0.00042355\tvalid_1's RMSPE: 0.195961\n",
      "[1100]\ttraining's rmse: 0.000364513\ttraining's RMSPE: 0.168715\tvalid_1's rmse: 0.000423308\tvalid_1's RMSPE: 0.195849\n",
      "[1150]\ttraining's rmse: 0.000362399\ttraining's RMSPE: 0.167737\tvalid_1's rmse: 0.000422985\tvalid_1's RMSPE: 0.1957\n",
      "[1200]\ttraining's rmse: 0.000360464\ttraining's RMSPE: 0.166841\tvalid_1's rmse: 0.000422892\tvalid_1's RMSPE: 0.195657\n",
      "[1250]\ttraining's rmse: 0.000358615\ttraining's RMSPE: 0.165985\tvalid_1's rmse: 0.000422661\tvalid_1's RMSPE: 0.19555\n",
      "[1300]\ttraining's rmse: 0.00035683\ttraining's RMSPE: 0.165159\tvalid_1's rmse: 0.0004227\tvalid_1's RMSPE: 0.195568\n",
      "Early stopping, best iteration is:\n",
      "[1278]\ttraining's rmse: 0.000357599\ttraining's RMSPE: 0.165515\tvalid_1's rmse: 0.000422541\tvalid_1's RMSPE: 0.195495\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000475348\ttraining's RMSPE: 0.219663\tvalid_1's rmse: 0.000497168\tvalid_1's RMSPE: 0.231487\n",
      "[100]\ttraining's rmse: 0.000454349\ttraining's RMSPE: 0.209959\tvalid_1's rmse: 0.000480078\tvalid_1's RMSPE: 0.22353\n",
      "[150]\ttraining's rmse: 0.000441491\ttraining's RMSPE: 0.204018\tvalid_1's rmse: 0.000470003\tvalid_1's RMSPE: 0.218839\n",
      "[200]\ttraining's rmse: 0.000431673\ttraining's RMSPE: 0.199481\tvalid_1's rmse: 0.000463833\tvalid_1's RMSPE: 0.215966\n",
      "[250]\ttraining's rmse: 0.000423759\ttraining's RMSPE: 0.195823\tvalid_1's rmse: 0.000458342\tvalid_1's RMSPE: 0.213409\n",
      "[300]\ttraining's rmse: 0.000416792\ttraining's RMSPE: 0.192604\tvalid_1's rmse: 0.000453455\tvalid_1's RMSPE: 0.211134\n",
      "[350]\ttraining's rmse: 0.000411267\ttraining's RMSPE: 0.190051\tvalid_1's rmse: 0.000451128\tvalid_1's RMSPE: 0.21005\n",
      "[400]\ttraining's rmse: 0.00040591\ttraining's RMSPE: 0.187575\tvalid_1's rmse: 0.000448549\tvalid_1's RMSPE: 0.208849\n",
      "[450]\ttraining's rmse: 0.000401659\ttraining's RMSPE: 0.185611\tvalid_1's rmse: 0.000447278\tvalid_1's RMSPE: 0.208258\n",
      "[500]\ttraining's rmse: 0.000397317\ttraining's RMSPE: 0.183604\tvalid_1's rmse: 0.000445841\tvalid_1's RMSPE: 0.207588\n",
      "[550]\ttraining's rmse: 0.000393543\ttraining's RMSPE: 0.18186\tvalid_1's rmse: 0.000444708\tvalid_1's RMSPE: 0.207061\n",
      "[600]\ttraining's rmse: 0.000390267\ttraining's RMSPE: 0.180346\tvalid_1's rmse: 0.00044361\tvalid_1's RMSPE: 0.20655\n",
      "[650]\ttraining's rmse: 0.000387031\ttraining's RMSPE: 0.178851\tvalid_1's rmse: 0.000442356\tvalid_1's RMSPE: 0.205966\n",
      "[700]\ttraining's rmse: 0.000383707\ttraining's RMSPE: 0.177315\tvalid_1's rmse: 0.000440913\tvalid_1's RMSPE: 0.205294\n",
      "[750]\ttraining's rmse: 0.000381117\ttraining's RMSPE: 0.176118\tvalid_1's rmse: 0.000440354\tvalid_1's RMSPE: 0.205034\n",
      "[800]\ttraining's rmse: 0.000378185\ttraining's RMSPE: 0.174763\tvalid_1's rmse: 0.00043956\tvalid_1's RMSPE: 0.204664\n",
      "[850]\ttraining's rmse: 0.000375547\ttraining's RMSPE: 0.173544\tvalid_1's rmse: 0.000438676\tvalid_1's RMSPE: 0.204253\n",
      "[900]\ttraining's rmse: 0.000373021\ttraining's RMSPE: 0.172377\tvalid_1's rmse: 0.000437979\tvalid_1's RMSPE: 0.203928\n",
      "[950]\ttraining's rmse: 0.000370805\ttraining's RMSPE: 0.171353\tvalid_1's rmse: 0.000437751\tvalid_1's RMSPE: 0.203822\n",
      "[1000]\ttraining's rmse: 0.000368642\ttraining's RMSPE: 0.170353\tvalid_1's rmse: 0.000438542\tvalid_1's RMSPE: 0.20419\n",
      "Early stopping, best iteration is:\n",
      "[955]\ttraining's rmse: 0.000370612\ttraining's RMSPE: 0.171264\tvalid_1's rmse: 0.000437606\tvalid_1's RMSPE: 0.203754\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.000475985\ttraining's RMSPE: 0.220561\tvalid_1's rmse: 0.000484244\tvalid_1's RMSPE: 0.223016\n",
      "[100]\ttraining's rmse: 0.000454484\ttraining's RMSPE: 0.210598\tvalid_1's rmse: 0.000466605\tvalid_1's RMSPE: 0.214892\n",
      "[150]\ttraining's rmse: 0.000441469\ttraining's RMSPE: 0.204567\tvalid_1's rmse: 0.000457099\tvalid_1's RMSPE: 0.210514\n",
      "[200]\ttraining's rmse: 0.000432086\ttraining's RMSPE: 0.200219\tvalid_1's rmse: 0.000450965\tvalid_1's RMSPE: 0.207689\n",
      "[250]\ttraining's rmse: 0.000424936\ttraining's RMSPE: 0.196906\tvalid_1's rmse: 0.000446343\tvalid_1's RMSPE: 0.205561\n",
      "[300]\ttraining's rmse: 0.000417307\ttraining's RMSPE: 0.193371\tvalid_1's rmse: 0.000441139\tvalid_1's RMSPE: 0.203164\n",
      "[350]\ttraining's rmse: 0.00041143\ttraining's RMSPE: 0.190647\tvalid_1's rmse: 0.000437455\tvalid_1's RMSPE: 0.201467\n",
      "[400]\ttraining's rmse: 0.000406618\ttraining's RMSPE: 0.188418\tvalid_1's rmse: 0.000435325\tvalid_1's RMSPE: 0.200486\n",
      "[450]\ttraining's rmse: 0.000401627\ttraining's RMSPE: 0.186105\tvalid_1's rmse: 0.000432714\tvalid_1's RMSPE: 0.199284\n",
      "[500]\ttraining's rmse: 0.000397606\ttraining's RMSPE: 0.184242\tvalid_1's rmse: 0.000430667\tvalid_1's RMSPE: 0.198341\n",
      "[550]\ttraining's rmse: 0.000393688\ttraining's RMSPE: 0.182427\tvalid_1's rmse: 0.000428871\tvalid_1's RMSPE: 0.197514\n",
      "[600]\ttraining's rmse: 0.000390161\ttraining's RMSPE: 0.180792\tvalid_1's rmse: 0.000427461\tvalid_1's RMSPE: 0.196865\n",
      "[650]\ttraining's rmse: 0.000387066\ttraining's RMSPE: 0.179358\tvalid_1's rmse: 0.000426553\tvalid_1's RMSPE: 0.196447\n",
      "[700]\ttraining's rmse: 0.000383909\ttraining's RMSPE: 0.177895\tvalid_1's rmse: 0.00042533\tvalid_1's RMSPE: 0.195883\n",
      "[750]\ttraining's rmse: 0.000380982\ttraining's RMSPE: 0.176539\tvalid_1's rmse: 0.000424222\tvalid_1's RMSPE: 0.195373\n",
      "[800]\ttraining's rmse: 0.000378406\ttraining's RMSPE: 0.175345\tvalid_1's rmse: 0.000423266\tvalid_1's RMSPE: 0.194933\n",
      "[850]\ttraining's rmse: 0.00037584\ttraining's RMSPE: 0.174156\tvalid_1's rmse: 0.000422449\tvalid_1's RMSPE: 0.194557\n",
      "[900]\ttraining's rmse: 0.00037337\ttraining's RMSPE: 0.173011\tvalid_1's rmse: 0.00042183\tvalid_1's RMSPE: 0.194271\n",
      "[950]\ttraining's rmse: 0.000371119\ttraining's RMSPE: 0.171969\tvalid_1's rmse: 0.000421224\tvalid_1's RMSPE: 0.193992\n",
      "[1000]\ttraining's rmse: 0.00036881\ttraining's RMSPE: 0.170898\tvalid_1's rmse: 0.000420754\tvalid_1's RMSPE: 0.193776\n",
      "[1050]\ttraining's rmse: 0.00036672\ttraining's RMSPE: 0.16993\tvalid_1's rmse: 0.000420524\tvalid_1's RMSPE: 0.19367\n",
      "[1100]\ttraining's rmse: 0.000364599\ttraining's RMSPE: 0.168947\tvalid_1's rmse: 0.000419819\tvalid_1's RMSPE: 0.193345\n",
      "[1150]\ttraining's rmse: 0.000362358\ttraining's RMSPE: 0.167909\tvalid_1's rmse: 0.000419327\tvalid_1's RMSPE: 0.193119\n",
      "[1200]\ttraining's rmse: 0.000360412\ttraining's RMSPE: 0.167007\tvalid_1's rmse: 0.000418868\tvalid_1's RMSPE: 0.192907\n",
      "[1250]\ttraining's rmse: 0.000358643\ttraining's RMSPE: 0.166187\tvalid_1's rmse: 0.000418777\tvalid_1's RMSPE: 0.192865\n",
      "Early stopping, best iteration is:\n",
      "[1207]\ttraining's rmse: 0.000360175\ttraining's RMSPE: 0.166897\tvalid_1's rmse: 0.000418755\tvalid_1's RMSPE: 0.192855\n",
      "Our out of folds RMSPE is 0.19662426802565763\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate first WAP\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate second WAP\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate the log of the return\n",
    "# Remember that logb(x / y) = logb(x) - logb(y)\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "# Calculate the realized volatility\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "# Function to count unique elements of a series\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "# Function to read our base train and test set\n",
    "def read_train_test():\n",
    "    train = pd.read_csv(data_dir + 'train.csv')\n",
    "    test = pd.read_csv(data_dir + 'test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    return train, test\n",
    "\n",
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.mean, np.std],\n",
    "        'wap2': [np.sum, np.mean, np.std],\n",
    "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'wap_balance': [np.sum, np.mean, np.std],\n",
    "        'price_spread':[np.sum, np.mean, np.std],\n",
    "        'bid_spread':[np.sum, np.mean, np.std],\n",
    "        'ask_spread':[np.sum, np.mean, np.std],\n",
    "        'total_volume':[np.sum, np.mean, np.std],\n",
    "        'volume_imbalance':[np.sum, np.mean, np.std]\n",
    "    }\n",
    "    \n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature.loc[df_feature['log_return1_realized_volatility'] < 0.015]\n",
    "\n",
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.mean],\n",
    "    }\n",
    "    \n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    # Get realized volatility columns\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate(train, test):\n",
    "    # Hyperparammeters (just basic)\n",
    "#     params = {\n",
    "#       'objective': 'rmse',  \n",
    "#       'boosting_type': 'gbdt',\n",
    "#       'num_leaves': 100,\n",
    "#       'n_jobs': -1,\n",
    "#       'learning_rate': 0.1,\n",
    "#       'feature_fraction': 0.8,\n",
    "#       'bagging_fraction': 0.8,\n",
    "#       'verbose': -1\n",
    "#     }\n",
    "    params = {\n",
    "          'objective': 'rmse',  \n",
    "          'boosting_type': 'gbdt',\n",
    "          'n_jobs': -1,\n",
    "          'lambda_l1': 5.592247279758206e-07,\n",
    "          'lambda_l2': 1.1510403060955105e-06,\n",
    "          'num_leaves': 23,\n",
    "          'feature_fraction': 0.7342101509015705,\n",
    "          'bagging_fraction': 0.9850760296767042,\n",
    "          'bagging_freq': 6,\n",
    "          'min_child_samples': 97,\n",
    "          'verbose': -1\n",
    "        }\n",
    "    \n",
    "    # Split features and target\n",
    "    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n",
    "    \n",
    "    y = train['target']\n",
    "    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n",
    "    \n",
    "    # ===========================\n",
    "    # Select features\n",
    "    # ===========================\n",
    "    x = x[selected_feature_names]\n",
    "    x_test = x_test[selected_feature_names]\n",
    "    print(x.shape, x_test.shape)\n",
    "    \n",
    "    \n",
    "    # Transform stock id to a numeric value\n",
    "    x['stock_id'] = x['stock_id'].astype(int)\n",
    "    x_test['stock_id'] = x_test['stock_id'].astype(int)\n",
    "    \n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(x.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(x_test.shape[0])\n",
    "    # Create a KFold object\n",
    "    kfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n",
    "        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n",
    "        model = lgb.train(params = params, \n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          num_boost_round = 10000, \n",
    "                          early_stopping_rounds = 50, \n",
    "                          verbose_eval = 50,\n",
    "                          feval = feval_rmspe)\n",
    "        # Add predictions to the out of folds array\n",
    "        oof_predictions[val_ind] = model.predict(x_val)\n",
    "        # Predict the test set\n",
    "        test_predictions += model.predict(x_test) / 5\n",
    "        \n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
    "    # Return test predictions\n",
    "    return test_predictions\n",
    "\n",
    "# Read train and test\n",
    "train, test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "train_stock_ids = train['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "train = get_time_stock(train)\n",
    "test = get_time_stock(test)\n",
    "\n",
    "# Traing and evaluate\n",
    "test_predictions = train_and_evaluate(train, test)\n",
    "# Save test predictions\n",
    "test['target'] = test_predictions\n",
    "test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-proceeding",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
