{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spiritual-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "import scipy as sc\n",
    "from scipy.stats import skew, kurtosis, median_absolute_deviation\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "# data directory\n",
    "data_dir = '../../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "excess-patio",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=4)]: Done 112 out of 112 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "      <th>wap1_sum</th>\n",
       "      <th>wap1_mean</th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap1_amax</th>\n",
       "      <th>wap2_sum</th>\n",
       "      <th>wap2_mean</th>\n",
       "      <th>wap2_std</th>\n",
       "      <th>wap2_amax</th>\n",
       "      <th>...</th>\n",
       "      <th>trade_price_kurtosis</th>\n",
       "      <th>trade_seconds_in_bucket_count_unique</th>\n",
       "      <th>trade_size_mean</th>\n",
       "      <th>trade_size_sum</th>\n",
       "      <th>trade_size_std</th>\n",
       "      <th>trade_size_amax</th>\n",
       "      <th>trade_order_count_mean</th>\n",
       "      <th>trade_order_count_sum</th>\n",
       "      <th>trade_order_count_std</th>\n",
       "      <th>trade_order_count_amax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-5</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>303.125061</td>\n",
       "      <td>1.003725</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>1.004920</td>\n",
       "      <td>303.105530</td>\n",
       "      <td>1.003661</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>1.005124</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231690</td>\n",
       "      <td>40.0</td>\n",
       "      <td>79.475000</td>\n",
       "      <td>3179.0</td>\n",
       "      <td>118.375107</td>\n",
       "      <td>499.0</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2.467741</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-11</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>200.047775</td>\n",
       "      <td>1.000239</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>1.000834</td>\n",
       "      <td>200.041168</td>\n",
       "      <td>1.000206</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>1.001067</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.338660</td>\n",
       "      <td>30.0</td>\n",
       "      <td>42.966667</td>\n",
       "      <td>1289.0</td>\n",
       "      <td>77.815203</td>\n",
       "      <td>280.0</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.446756</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-16</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>187.913849</td>\n",
       "      <td>0.999542</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>1.000878</td>\n",
       "      <td>187.939819</td>\n",
       "      <td>0.999680</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>1.000876</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.930625</td>\n",
       "      <td>25.0</td>\n",
       "      <td>86.440000</td>\n",
       "      <td>2161.0</td>\n",
       "      <td>113.587000</td>\n",
       "      <td>391.0</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2.300725</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0-31</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>119.859779</td>\n",
       "      <td>0.998832</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>1.000412</td>\n",
       "      <td>119.835945</td>\n",
       "      <td>0.998633</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>1.000116</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.979349</td>\n",
       "      <td>15.0</td>\n",
       "      <td>130.800000</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>144.828569</td>\n",
       "      <td>450.0</td>\n",
       "      <td>3.933333</td>\n",
       "      <td>59.0</td>\n",
       "      <td>4.043808</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0-62</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>175.932861</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>1.000159</td>\n",
       "      <td>175.934250</td>\n",
       "      <td>0.999626</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>1.000249</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065389</td>\n",
       "      <td>22.0</td>\n",
       "      <td>81.409091</td>\n",
       "      <td>1791.0</td>\n",
       "      <td>117.914682</td>\n",
       "      <td>341.0</td>\n",
       "      <td>4.045455</td>\n",
       "      <td>89.0</td>\n",
       "      <td>4.099678</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id    target    wap1_sum  wap1_mean  wap1_std  wap1_amax    wap2_sum  \\\n",
       "0    0-5  0.004136  303.125061   1.003725  0.000693   1.004920  303.105530   \n",
       "1   0-11  0.001445  200.047775   1.000239  0.000262   1.000834  200.041168   \n",
       "2   0-16  0.002168  187.913849   0.999542  0.000864   1.000878  187.939819   \n",
       "3   0-31  0.002195  119.859779   0.998832  0.000757   1.000412  119.835945   \n",
       "4   0-62  0.001747  175.932861   0.999619  0.000258   1.000159  175.934250   \n",
       "\n",
       "   wap2_mean  wap2_std  wap2_amax  ...  trade_price_kurtosis  \\\n",
       "0   1.003661  0.000781   1.005124  ...             -0.231690   \n",
       "1   1.000206  0.000272   1.001067  ...             -0.338660   \n",
       "2   0.999680  0.000862   1.000876  ...             -0.930625   \n",
       "3   0.998633  0.000656   1.000116  ...             -0.979349   \n",
       "4   0.999626  0.000317   1.000249  ...             -0.065389   \n",
       "\n",
       "   trade_seconds_in_bucket_count_unique  trade_size_mean  trade_size_sum  \\\n",
       "0                                  40.0        79.475000          3179.0   \n",
       "1                                  30.0        42.966667          1289.0   \n",
       "2                                  25.0        86.440000          2161.0   \n",
       "3                                  15.0       130.800000          1962.0   \n",
       "4                                  22.0        81.409091          1791.0   \n",
       "\n",
       "   trade_size_std  trade_size_amax  trade_order_count_mean  \\\n",
       "0      118.375107            499.0                2.750000   \n",
       "1       77.815203            280.0                1.900000   \n",
       "2      113.587000            391.0                2.720000   \n",
       "3      144.828569            450.0                3.933333   \n",
       "4      117.914682            341.0                4.045455   \n",
       "\n",
       "   trade_order_count_sum  trade_order_count_std  trade_order_count_amax  \n",
       "0                  110.0               2.467741                    12.0  \n",
       "1                   57.0               1.446756                     6.0  \n",
       "2                   68.0               2.300725                     8.0  \n",
       "3                   59.0               4.043808                    15.0  \n",
       "4                   89.0               4.099678                    17.0  \n",
       "\n",
       "[5 rows x 73 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])/(df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])/(df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1']*df['bid_size1'] + df['bid_price2']*df['ask_size2'] + df['ask_price2']*df['bid_size2']) / (\n",
    "                            df['ask_size1'] + df['bid_size1'] + df['ask_size2'] + df['bid_size2'])\n",
    "    return wap\n",
    "\n",
    "def log_return(list_stock_prices):\n",
    "    return np.log(list_stock_prices).diff() \n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "\n",
    "def preprocessor_book(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['wap3'] = calc_wap3(df)\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return).fillna(0)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return).fillna(0)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return).fillna(0)\n",
    "    \n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.mean, np.std,np.max],\n",
    "        'wap2': [np.sum, np.mean, np.std,np.max],\n",
    "        'wap3': [np.sum, np.mean, np.std,np.max],\n",
    "        'log_return1': [np.sum, np.mean, np.std,np.max, realized_volatility],\n",
    "        'log_return2': [np.sum, np.mean, np.std,np.max, realized_volatility],\n",
    "        'log_return3': [np.sum, np.mean, np.std,np.max, realized_volatility],\n",
    "        'ask_price1':[np.sum, np.mean, np.std,np.max],\n",
    "        'ask_price2':[np.sum, np.mean, np.std,np.max],\n",
    "        'bid_price1':[np.sum, np.mean, np.std,np.max],\n",
    "        'bid_price2':[np.sum, np.mean, np.std,np.max],\n",
    "        'total_volume':[np.sum, np.mean, np.std,np.max],\n",
    "    }\n",
    "\n",
    "    #####groupby / all seconds\n",
    "    df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n",
    "    \n",
    "    df_feature.columns = ['_'.join(col) for col in df_feature.columns] #time_id is changed to time_id_\n",
    "    #create row_id\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature = df_feature.drop(['time_id_'],axis=1)\n",
    "    \n",
    "    return df_feature\n",
    "\n",
    "\n",
    "def preprocessor_trade(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return).fillna(0)\n",
    "    \n",
    "    \n",
    "    aggregate_dictionary = {\n",
    "        'log_return':[np.sum, realized_volatility, np.mean, np.std,np.max, median_absolute_deviation,skew,kurtosis],\n",
    "        'price':[np.sum, np.mean, np.std, np.max, median_absolute_deviation,skew,kurtosis],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.mean, np.sum, np.std, np.max],\n",
    "        'order_count':[np.mean, np.sum, np.std, np.max],\n",
    "    }\n",
    "    \n",
    "    df_feature = df.groupby('time_id').agg(aggregate_dictionary)\n",
    "    \n",
    "    df_feature = df_feature.reset_index()\n",
    "    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature = df_feature.drop(['trade_time_id_'],axis=1)\n",
    "    \n",
    "    return df_feature\n",
    "\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    from joblib import Parallel, delayed # parallel computing to save time\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    def for_joblib(stock_id):\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "            \n",
    "        df_tmp = pd.merge(preprocessor_book(file_path_book),preprocessor_trade(file_path_trade),on='row_id',how='left')\n",
    "     \n",
    "        return pd.concat([df,df_tmp])\n",
    "    \n",
    "    df = Parallel(n_jobs=4, verbose=1)(\n",
    "        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n",
    "        )\n",
    "\n",
    "    df =  pd.concat(df,ignore_index = True)\n",
    "    return df\n",
    "\n",
    "train = pd.read_csv(data_dir + 'train.csv')\n",
    "train_ids = train.stock_id.unique()\n",
    "df_train = preprocessor(list_stock_ids= train_ids, is_train = True).fillna(-999)\n",
    "train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "train = train[['row_id','target']]\n",
    "df_train = train.merge(df_train, on = ['row_id'], how = 'left')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "knowing-guess",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 1\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037542 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18472\n",
      "[LightGBM] [Info] Number of data points in the train set: 386038, number of used features: 73\n",
      "[LightGBM] [Info] Start training from score 0.001803\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000481755\ttraining's RMSPE: 0.22273\tvalid_1's rmse: 0.000500173\tvalid_1's RMSPE: 0.23361\n",
      "[200]\ttraining's rmse: 0.000467321\ttraining's RMSPE: 0.21606\tvalid_1's rmse: 0.000500889\tvalid_1's RMSPE: 0.23394\n",
      "Early stopping, best iteration is:\n",
      "[120]\ttraining's rmse: 0.000478285\ttraining's RMSPE: 0.22113\tvalid_1's rmse: 0.000499496\tvalid_1's RMSPE: 0.23329\n",
      "Performance of the　prediction: , RMSPE: 0.233\n",
      "****************************************************************************************************\n",
      "Fold : 2\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050767 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18472\n",
      "[LightGBM] [Info] Number of data points in the train set: 386038, number of used features: 73\n",
      "[LightGBM] [Info] Start training from score 0.001797\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000481031\ttraining's RMSPE: 0.22285\tvalid_1's rmse: 0.000502813\tvalid_1's RMSPE: 0.23064\n",
      "[200]\ttraining's rmse: 0.000466513\ttraining's RMSPE: 0.21612\tvalid_1's rmse: 0.000501438\tvalid_1's RMSPE: 0.23001\n",
      "[300]\ttraining's rmse: 0.000455661\ttraining's RMSPE: 0.21109\tvalid_1's rmse: 0.000501809\tvalid_1's RMSPE: 0.23018\n",
      "Early stopping, best iteration is:\n",
      "[228]\ttraining's rmse: 0.000463294\ttraining's RMSPE: 0.21463\tvalid_1's rmse: 0.00050129\tvalid_1's RMSPE: 0.22994\n",
      "Performance of the　prediction: , RMSPE: 0.23\n",
      "****************************************************************************************************\n",
      "Fold : 3\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18472\n",
      "[LightGBM] [Info] Number of data points in the train set: 386039, number of used features: 73\n",
      "[LightGBM] [Info] Start training from score 0.001798\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000481065\ttraining's RMSPE: 0.22284\tvalid_1's rmse: 0.000504414\tvalid_1's RMSPE: 0.23154\n",
      "[200]\ttraining's rmse: 0.000466814\ttraining's RMSPE: 0.21624\tvalid_1's rmse: 0.000503537\tvalid_1's RMSPE: 0.23113\n",
      "[300]\ttraining's rmse: 0.000456048\ttraining's RMSPE: 0.21126\tvalid_1's rmse: 0.000503724\tvalid_1's RMSPE: 0.23122\n",
      "Early stopping, best iteration is:\n",
      "[277]\ttraining's rmse: 0.000458273\ttraining's RMSPE: 0.21229\tvalid_1's rmse: 0.000503325\tvalid_1's RMSPE: 0.23103\n",
      "Performance of the　prediction: , RMSPE: 0.231\n",
      "****************************************************************************************************\n",
      "Fold : 4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18472\n",
      "[LightGBM] [Info] Number of data points in the train set: 386039, number of used features: 73\n",
      "[LightGBM] [Info] Start training from score 0.001798\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000481087\ttraining's RMSPE: 0.22288\tvalid_1's rmse: 0.000508424\tvalid_1's RMSPE: 0.2331\n",
      "[200]\ttraining's rmse: 0.000466605\ttraining's RMSPE: 0.21617\tvalid_1's rmse: 0.000507789\tvalid_1's RMSPE: 0.23281\n",
      "Early stopping, best iteration is:\n",
      "[181]\ttraining's rmse: 0.00046894\ttraining's RMSPE: 0.21726\tvalid_1's rmse: 0.000507183\tvalid_1's RMSPE: 0.23253\n",
      "Performance of the　prediction: , RMSPE: 0.233\n",
      "****************************************************************************************************\n",
      "Fold : 5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18472\n",
      "[LightGBM] [Info] Number of data points in the train set: 386039, number of used features: 73\n",
      "[LightGBM] [Info] Start training from score 0.001798\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000480881\ttraining's RMSPE: 0.2227\tvalid_1's rmse: 0.000504126\tvalid_1's RMSPE: 0.23194\n",
      "[200]\ttraining's rmse: 0.000466623\ttraining's RMSPE: 0.2161\tvalid_1's rmse: 0.00050292\tvalid_1's RMSPE: 0.23138\n",
      "[300]\ttraining's rmse: 0.000455961\ttraining's RMSPE: 0.21116\tvalid_1's rmse: 0.000503191\tvalid_1's RMSPE: 0.23151\n",
      "Early stopping, best iteration is:\n",
      "[254]\ttraining's rmse: 0.000460595\ttraining's RMSPE: 0.21331\tvalid_1's rmse: 0.000502604\tvalid_1's RMSPE: 0.23124\n",
      "Performance of the　prediction: , RMSPE: 0.231\n",
      "****************************************************************************************************\n",
      "Fold : 6\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18472\n",
      "[LightGBM] [Info] Number of data points in the train set: 386039, number of used features: 73\n",
      "[LightGBM] [Info] Start training from score 0.001802\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.00048159\ttraining's RMSPE: 0.22273\tvalid_1's rmse: 0.000513573\tvalid_1's RMSPE: 0.23915\n",
      "Early stopping, best iteration is:\n",
      "[51]\ttraining's rmse: 0.000493853\ttraining's RMSPE: 0.2284\tvalid_1's rmse: 0.000510799\tvalid_1's RMSPE: 0.23786\n",
      "Performance of the　prediction: , RMSPE: 0.238\n",
      "****************************************************************************************************\n",
      "Fold : 7\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18472\n",
      "[LightGBM] [Info] Number of data points in the train set: 386039, number of used features: 73\n",
      "[LightGBM] [Info] Start training from score 0.001801\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000481239\ttraining's RMSPE: 0.22259\tvalid_1's rmse: 0.000504157\tvalid_1's RMSPE: 0.23457\n",
      "[200]\ttraining's rmse: 0.000466916\ttraining's RMSPE: 0.21597\tvalid_1's rmse: 0.000505454\tvalid_1's RMSPE: 0.23517\n",
      "Early stopping, best iteration is:\n",
      "[104]\ttraining's rmse: 0.000480514\ttraining's RMSPE: 0.22226\tvalid_1's rmse: 0.00050375\tvalid_1's RMSPE: 0.23438\n",
      "Performance of the　prediction: , RMSPE: 0.234\n",
      "****************************************************************************************************\n",
      "Fold : 8\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18472\n",
      "[LightGBM] [Info] Number of data points in the train set: 386039, number of used features: 73\n",
      "[LightGBM] [Info] Start training from score 0.001803\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000481677\ttraining's RMSPE: 0.22275\tvalid_1's rmse: 0.000516635\tvalid_1's RMSPE: 0.24075\n",
      "Early stopping, best iteration is:\n",
      "[59]\ttraining's rmse: 0.000490868\ttraining's RMSPE: 0.22701\tvalid_1's rmse: 0.000513623\tvalid_1's RMSPE: 0.23935\n",
      "Performance of the　prediction: , RMSPE: 0.239\n",
      "****************************************************************************************************\n",
      "Fold : 9\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18472\n",
      "[LightGBM] [Info] Number of data points in the train set: 386039, number of used features: 73\n",
      "[LightGBM] [Info] Start training from score 0.001799\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000481217\ttraining's RMSPE: 0.22279\tvalid_1's rmse: 0.000515131\tvalid_1's RMSPE: 0.23765\n",
      "[200]\ttraining's rmse: 0.000466697\ttraining's RMSPE: 0.21607\tvalid_1's rmse: 0.00051378\tvalid_1's RMSPE: 0.23703\n",
      "[300]\ttraining's rmse: 0.000455859\ttraining's RMSPE: 0.21105\tvalid_1's rmse: 0.000513085\tvalid_1's RMSPE: 0.23671\n",
      "Early stopping, best iteration is:\n",
      "[249]\ttraining's rmse: 0.00046111\ttraining's RMSPE: 0.21348\tvalid_1's rmse: 0.000512503\tvalid_1's RMSPE: 0.23644\n",
      "Performance of the　prediction: , RMSPE: 0.236\n",
      "****************************************************************************************************\n",
      "Fold : 10\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044257 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18472\n",
      "[LightGBM] [Info] Number of data points in the train set: 386039, number of used features: 73\n",
      "[LightGBM] [Info] Start training from score 0.001803\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.000481142\ttraining's RMSPE: 0.22249\tvalid_1's rmse: 0.000510302\tvalid_1's RMSPE: 0.238\n",
      "Early stopping, best iteration is:\n",
      "[84]\ttraining's rmse: 0.000484278\ttraining's RMSPE: 0.22394\tvalid_1's rmse: 0.000510011\tvalid_1's RMSPE: 0.23787\n",
      "Performance of the　prediction: , RMSPE: 0.238\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(data_dir + 'test.csv')\n",
    "test_ids = test.stock_id.unique()\n",
    "df_test = preprocessor(list_stock_ids= test_ids, is_train = False)\n",
    "df_test = test.merge(df_test, on = ['row_id'], how = 'left')\n",
    "\n",
    "#stock_id target encoding\n",
    "df_train['stock_id'] = df_train['row_id'].apply(lambda x:x.split('-')[0])\n",
    "df_test['stock_id'] = df_test['row_id'].apply(lambda x:x.split('-')[0])\n",
    "\n",
    "stock_id_target_mean = df_train.groupby('stock_id')['target'].mean() \n",
    "df_test['stock_id_target_enc'] = df_test['stock_id'].map(stock_id_target_mean) # test_set\n",
    "\n",
    "#training\n",
    "tmp = np.repeat(np.nan, df_train.shape[0])\n",
    "kf = KFold(n_splits = 10, shuffle=True,random_state = 19911109)\n",
    "for idx_1, idx_2 in kf.split(df_train):\n",
    "    target_mean = df_train.iloc[idx_1].groupby('stock_id')['target'].mean()\n",
    "\n",
    "    tmp[idx_2] = df_train['stock_id'].iloc[idx_2].map(target_mean)\n",
    "df_train['stock_id_target_enc'] = tmp\n",
    "\n",
    "DO_FEAT_IMP = False\n",
    "if len(df_test)==3:\n",
    "    DO_FEAT_IMP = True\n",
    "    \n",
    "# ref https://www.kaggle.com/corochann/permutation-importance-for-feature-selection-part1\n",
    "def calc_model_importance(model, feature_names=None, importance_type='gain'):\n",
    "    importance_df = pd.DataFrame(model.feature_importance(importance_type=importance_type),\n",
    "                                 index=feature_names,\n",
    "                                 columns=['importance']).sort_values('importance')\n",
    "    return importance_df\n",
    "\n",
    "\n",
    "def plot_importance(importance_df, title='',\n",
    "                    save_filepath=None, figsize=(8, 12)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    importance_df.plot.barh(ax=ax)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if save_filepath is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(save_filepath)\n",
    "    plt.close()\n",
    "    \n",
    "df_train['stock_id'] = df_train['stock_id'].astype(int)\n",
    "df_test['stock_id'] = df_test['stock_id'].astype(int)\n",
    "\n",
    "X = df_train.drop(['row_id','target'],axis=1)\n",
    "y = df_train['target']\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "def feval_RMSPE(preds, lgbm_train):\n",
    "    labels = lgbm_train.get_label()\n",
    "    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n",
    "\n",
    "params = {\n",
    "    'objective': 'rmse',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'early_stopping_rounds': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'lambda_l1': 0.4311,\n",
    "    'lambda_l2': 0.3897,\n",
    "    'num_leaves': 77,\n",
    "    'feature_fraction': 0.45,\n",
    "    'bagging_fraction': 0.793,\n",
    "    'bagging_freq': 1,\n",
    "    'min_child_samples': 96,\n",
    "  }\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=19901028, shuffle=True)\n",
    "oof = pd.DataFrame()                 # out-of-fold result\n",
    "models = []                          # models\n",
    "scores = 0.0                         # validation score\n",
    "\n",
    "gain_importance_list = []\n",
    "split_importance_list = []\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "\n",
    "    print(\"Fold :\", fold+1)\n",
    "    \n",
    "    # create dataset\n",
    "    X_train, y_train = X.loc[trn_idx], y[trn_idx]\n",
    "    X_valid, y_valid = X.loc[val_idx], y[val_idx]\n",
    "    \n",
    "    #RMSPE weight\n",
    "    weights = 1/np.square(y_train)\n",
    "    lgbm_train = lgb.Dataset(X_train,y_train,weight = weights)\n",
    "\n",
    "    weights = 1/np.square(y_valid)\n",
    "    lgbm_valid = lgb.Dataset(X_valid,y_valid,reference = lgbm_train,weight = weights)\n",
    "    \n",
    "    # model \n",
    "    model = lgb.train(params=params,\n",
    "                      train_set=lgbm_train,\n",
    "                      valid_sets=[lgbm_train, lgbm_valid],\n",
    "                      num_boost_round=5000,         \n",
    "                      feval=feval_RMSPE,\n",
    "                      verbose_eval=100,\n",
    "                      categorical_feature = ['stock_id']                \n",
    "                     )\n",
    "    \n",
    "    # validation \n",
    "    y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "\n",
    "    RMSPE = round(rmspe(y_true = y_valid, y_pred = y_pred),3)\n",
    "    print(f'Performance of the　prediction: , RMSPE: {RMSPE}')\n",
    "\n",
    "    #keep scores and models\n",
    "    scores += RMSPE / 10\n",
    "    models.append(model)\n",
    "    print(\"*\" * 100)\n",
    "    \n",
    "    # --- calc model feature importance ---\n",
    "    if DO_FEAT_IMP:    \n",
    "        feature_names = X_train.columns.values.tolist()\n",
    "        gain_importance_df = calc_model_importance(\n",
    "            model, feature_names=feature_names, importance_type='gain')\n",
    "        gain_importance_list.append(gain_importance_df)\n",
    "\n",
    "        split_importance_df = calc_model_importance(\n",
    "            model, feature_names=feature_names, importance_type='split')\n",
    "        split_importance_list.append(split_importance_df)\n",
    "print(f'Our out of folds RMSPE is {rmspe_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "owned-boards",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2343"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-cache",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
